# Programmation dynamique

## 1. Introduction

La **programmation dynamique** (ou **dynamic programming**, abr√©g√©e **DP**) est une m√©thode algorithmique puissante permettant de **r√©soudre efficacement des probl√®mes complexes en les d√©composant en sous-probl√®mes plus simples**.

Elle est particuli√®rement efficace lorsqu‚Äôun probl√®me poss√®de deux propri√©t√©s fondamentales :

- Des **sous-probl√®mes qui se recoupent** : les m√™mes calculs apparaissent plusieurs fois dans une approche na√Øve (souvent r√©cursive).
- Une **structure optimale** : une solution optimale peut √™tre construite √† partir des solutions optimales de sous-probl√®mes (principe d‚Äô**optimalit√© de Bellman**).

Plut√¥t que de recalculer plusieurs fois les m√™mes r√©sultats, la programmation dynamique **m√©morise** les solutions d√©j√† obtenues, soit :

- **en les stockant** (dans un tableau, un dictionnaire‚Ä¶) pour les r√©utiliser (**m√©mo√Øsation**, approche r√©cursive optimis√©e),
- **en les calculant directement dans un certain ordre**, souvent de bas en haut (**programmation tabulaire**, approche it√©rative).

Cette m√©thode est tr√®s utilis√©e pour :

- les **probl√®mes d‚Äôoptimisation**
- les **probl√®mes de d√©nombrement**
- les **probl√®mes de d√©cision combinatoire**

## 2. Quand utiliser la programmation dynamique ?

Un probl√®me peut √™tre r√©solu efficacement par programmation dynamique s‚Äôil satisfait **les deux crit√®res suivants** :

### 2.1. Sous-probl√®mes qui se recoupent

- Un **m√™me sous-probl√®me** revient plusieurs fois dans l‚Äôalgorithme na√Øf (souvent de mani√®re exponentielle).
- Typiquement observ√© dans les **algorithmes r√©cursifs** na√Øfs o√π l‚Äôarbre d‚Äôappel contient beaucoup de **r√©p√©titions inutiles**.

_Exemple :_  
Dans le calcul na√Øf de la suite de Fibonacci, `F(3)` est appel√© plusieurs fois par des branches diff√©rentes.

### 2.2. Structure optimale (principe d‚Äôoptimalit√©)

- Une solution **optimale globale** peut √™tre **construite √† partir des solutions optimales des sous-probl√®mes**.
- Autrement dit, **prendre la meilleure d√©cision √† chaque √©tape locale** permet d‚Äôatteindre la solution globale (ce qui **n‚Äôest pas toujours vrai**, d‚Äôo√π l‚Äôint√©r√™t du test pr√©alable).

_Exemple :_  
Dans le probl√®me du sac √† dos 0/1, la meilleure valeur obtenue pour une capacit√© donn√©e repose sur les **meilleures valeurs pr√©c√©dentes**.

### 2.3. Autres indices utiles

- Le probl√®me peut se **d√©crire comme une fonction avec param√®tres entiers** (poids, index, taille, etc.).
- Le nombre total d‚Äô√©tats possibles est **polynomialement born√©** (ex : `n √ó W`, `n¬≤`, etc.).
- Une **formule de r√©currence** peut √™tre d√©finie pour relier un √©tat √† d‚Äôautres.

## 3. Principe de fonctionnement

La **programmation dynamique** consiste √† **r√©soudre un probl√®me en construisant progressivement la solution √† partir de sous-probl√®mes plus simples**, tout en **√©vitant les redondances** gr√¢ce √† la **m√©morisation** des r√©sultats d√©j√† calcul√©s.

Elle repose sur une **structure d‚Äô√©tat** (qu‚Äôon doit identifier), et une **formule de transition** (ou r√©currence) qui permet de **faire avancer le calcul**.

### 3.1. √âtapes g√©n√©rales

Voici les 5 grandes √©tapes d‚Äôun algorithme dynamique bien construit :

1. **D√©finir l‚Äô√©tat dynamique**  
   ‚Üí Quels param√®tres d√©crivent un sous-probl√®me ?  
   Exemple : `dp[i][w] = valeur maximale possible avec les i premiers objets et capacit√© w`

2. **√âtablir la relation de transition (r√©currence)**  
   ‚Üí Comment obtenir `dp[i]` √† partir des `dp[j]` plus petits ?  
   Exemple : `dp[i] = max(dp[i-1], dp[i-2] + valeur[i])`

3. **Initialiser les cas de base**  
   ‚Üí Que vaut la solution pour les cas les plus simples ?  
   Exemple : `dp[0] = 0`, `dp[1] = valeur[1]`, etc.

4. **Choisir l‚Äôordre de parcours**  
   ‚Üí **Top-down (m√©mo√Øsation)** ou **Bottom-up (tableau)**  
   On choisit l‚Äôapproche selon la clart√© du probl√®me ou les contraintes de performance.

5. **Lire la solution finale**  
   ‚Üí La r√©ponse se trouve g√©n√©ralement dans `dp[n]` ou `dp[n][k]`, selon les cas.

### 3.2. Deux approches fondamentales

#### a) Top-down (r√©cursif avec m√©mo√Øsation)

- On √©crit une fonction r√©cursive `f(i)` qui appelle ses sous-probl√®mes
- On utilise une **structure m√©moire** (ex : dictionnaire, tableau) pour ne pas recalculer un √©tat d√©j√† connu
- C‚Äôest une **approche naturelle** si le raisonnement est r√©cursif

üìå _Exemple :_

```python
memo = {}
def f(i):
    if i in memo:
        return memo[i]
    # cas de base
    ...
    # appel r√©cursif
    memo[i] = f(...) + ...
    return memo[i]
```

#### b) Bottom-up (tabulaire)

- On **remplit un tableau** de mani√®re it√©rative, en **respectant l‚Äôordre des d√©pendances**
- On commence par les **cas de base**, puis on construit progressivement vers le haut
- C‚Äôest souvent **plus rapide** et **plus stable** (pas de r√©cursion)

üìå _Exemple :_

```python
dp = [0] * (n + 1)
dp[0] = ...
for i in range(1, n + 1):
    dp[i] = ...
```

### 3.3. Repr√©sentation visuelle

Voici une repr√©sentation g√©n√©rique d‚Äôun **tableau dynamique** utilis√© en bottom-up :

```
√âtat : dp[i] = solution au sous-probl√®me i

i     :   0     1     2     3     4     5
dp[i] :   0    ...   ...   ...   ...   ?
                     ‚Üë     ‚Üë     ‚Üë
           d√©pend de ces valeurs
```

> Comprendre la **structure des d√©pendances** entre √©tats est **cl√©** dans tout algorithme dynamique.

## 4. Exemple complet : suite de Fibonacci

### √ânonc√©

Calculer `F(n)`, la `n`-i√®me valeur de la suite de Fibonacci d√©finie par :

```
F(0) = 0
F(1) = 1
F(n) = F(n - 1) + F(n - 2)  pour n ‚â• 2
```

Ce probl√®me classique est id√©al pour illustrer le passage d‚Äôune solution r√©cursive inefficace √† une version optimis√©e via la programmation dynamique.

### Version r√©cursive simple (inefficace)

```python
def fib_naif(n):
    if n <= 1:
        return n
    return fib_naif(n - 1) + fib_naif(n - 2)
```

üî¥ **Probl√®me** :

- L‚Äôarbre des appels r√©cursifs contient √©norm√©ment de **redondances** (les m√™mes appels sont faits des milliers de fois).
- La complexit√© est **exponentielle** : `O(2^n)`

### Version top-down avec m√©mo√Øsation

```python
def fib_memo(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 1:
        memo[n] = n
    else:
        memo[n] = fib_memo(n - 1, memo) + fib_memo(n - 2, memo)
    return memo[n]
```

‚úÖ **Avantages** :

- √âvite les appels redondants : chaque `fib(k)` est calcul√© **au plus une fois**.
- Complexit√© r√©duite √† **O(n)**

üìå **Remarque** : On utilise ici un **dictionnaire** `memo` partag√© entre appels pour stocker les r√©sultats interm√©diaires.

### Version bottom-up (it√©rative)

```python
def fib_iter(n):
    if n <= 1:
        return n
    f = [0, 1]
    for i in range(2, n + 1):
        f.append(f[i - 1] + f[i - 2])
    return f[n]
```

‚úÖ **Avantages** :

- Aucun appel r√©cursif ‚Üí plus stable, plus rapide
- Complexit√© en **temps** : `O(n)`
- Complexit√© en **espace** : `O(n)` (car on garde tous les termes)

### Version optimis√©e en espace (`O(1)`)

Comme chaque terme d√©pend uniquement des deux pr√©c√©dents, on peut **r√©duire l‚Äôespace m√©moire √† deux variables** :

```python
def fib_opt(n):
    if n <= 1:
        return n
    a, b = 0, 1
    for _ in range(2, n + 1):
        a, b = b, a + b
    return b
```

‚úÖ **Avantage** :

- M√™me complexit√© en temps `O(n)`
- Complexit√© en espace **constante** `O(1)`

### Comparatif des versions

| Version | Temps | M√©moire | Stabilit√© | Remarques |
||--|--|--||
| R√©cursive na√Øve | `O(2^n)` | `O(n)` | ‚ùå | Tr√®s lent d√®s `n > 30` |
| Top-down m√©mo | `O(n)` | `O(n)` | ‚úÖ | Simple et efficace |
| Bottom-up | `O(n)` | `O(n)` | ‚úÖ | Sans r√©cursion |
| Optimis√©e (2 vars)| `O(n)` | `O(1)` | ‚úÖ‚úÖ | Version recommand√©e en pratique |

## 5. Exemple plus avanc√© : Probl√®me du sac √† dos 0/1

### √ânonc√©

On dispose de `n` objets, chacun ayant :

- un **poids** `poids[i]`
- une **valeur** `valeurs[i]`

On souhaite remplir un **sac de capacit√© maximale `W`** en choisissant certains objets **sans les fractionner**, de sorte que :

- la **valeur totale** des objets s√©lectionn√©s soit **maximale**
- le **poids total** ne d√©passe pas `W`

> On parle de **sac √† dos 0/1** car on prend **chaque objet en entier (1)** ou pas du tout (0).

### D√©finition de l‚Äô√©tat

On d√©finit une table `dp[i][j]` o√π :

- `i` est le **nombre d‚Äôobjets consid√©r√©s** (de 0 √† `n`)
- `j` est la **capacit√© du sac restante** (de 0 √† `W`)
- `dp[i][j]` repr√©sente la **valeur maximale** que l'on peut obtenir **avec les `i` premiers objets et une capacit√© `j`**

### Formule de r√©currence

Pour chaque objet `i` et chaque capacit√© `j`, on a deux choix :

1. **Ne pas prendre l‚Äôobjet `i`** : on garde la valeur pr√©c√©dente ‚Üí `dp[i-1][j]`
2. **Prendre l‚Äôobjet `i`** (si son poids est ‚â§ `j`) :
   - on ajoute sa valeur
   - on d√©duit son poids de la capacit√©
   - on ajoute sa valeur √† `dp[i-1][j - poids[i-1]]`

D‚Äôo√π la formule :

```
dp[i][j] = max(
    dp[i-1][j],                                 # sans l‚Äôobjet i
    dp[i-1][j - poids[i-1]] + valeurs[i-1]      # avec l‚Äôobjet i
)  si poids[i-1] ‚â§ j
```

Sinon :

```
dp[i][j] = dp[i-1][j]      # on ne peut pas prendre l‚Äôobjet i
```

### Impl√©mentation (bottom-up)

```python
def sac_a_dos(valeurs, poids, W):
    n = len(valeurs)
    dp = [[0] * (W + 1) for _ in range(n + 1)]

    for i in range(1, n + 1):
        for w in range(W + 1):
            if poids[i - 1] <= w:
                dp[i][w] = max(
                    dp[i - 1][w],
                    dp[i - 1][w - poids[i - 1]] + valeurs[i - 1]
                )
            else:
                dp[i][w] = dp[i - 1][w]

    return dp[n][W]
```

### Exemple concret

```python
valeurs = [60, 100, 120]
poids   = [10, 20, 30]
W       = 50

print(sac_a_dos(valeurs, poids, W))  # R√©sultat : 220
```

üì¶ On prend les objets 2 (100) et 3 (120) ‚Üí total 220 pour un poids de 50.

### Visualisation (extrait de tableau `dp`)

```
dp[i][j] ‚Üí valeur maximale avec les i premiers objets et capacit√© j

     j ‚Üí     0   1   2  ...  19  20  21  ...  50
i
‚Üì
0         [ 0,  0,  0, ... ,  0,  0,  0, ... ,  0 ]
1 (obj1)  [ 0,  0,  0, ... ,  0, 60, 60, ... , 60 ]
2 (obj2)  [ 0,  0,  0, ... , 60,100,100,... ,160 ]
3 (obj3)  [ 0,  0,  0, ... ,100,120,120,... ,220 ]
```

### Complexit√©

| Aspect | Valeur |
||-|
| Temps | `O(n √ó W)` |
| M√©moire | `O(n √ó W)` |
| Type | Programmation dynamique tabulaire |
| Possibilit√© d‚Äôoptimisation | Oui, on peut r√©duire la m√©moire √† `O(W)` en utilisant 1D |

### Optimisation m√©moire

Comme on n‚Äôutilise que la ligne `i-1` √† chaque it√©ration, on peut **remplacer la matrice 2D par un tableau 1D**, parcouru **√† l‚Äôenvers** :

```python
def sac_a_dos_opt(valeurs, poids, W):
    n = len(valeurs)
    dp = [0] * (W + 1)

    for i in range(n):
        for w in range(W, poids[i] - 1, -1):
            dp[w] = max(dp[w], dp[w - poids[i]] + valeurs[i])

    return dp[W]
```

## 6. Exemple avanc√© : Rendu de monnaie (nombre de fa√ßons)

### √ânonc√©

On dispose d‚Äôun ensemble de **pi√®ces de diff√©rentes valeurs** (par exemple : `[1, 2, 5]`)  
On souhaite d√©terminer **de combien de fa√ßons distinctes** on peut rendre une somme `S`, **en utilisant un nombre illimit√© de pi√®ces**.

> Ce probl√®me est un classique de **d√©nombrement avec r√©p√©tition**, et non d‚Äôoptimisation.

### D√©finition de l‚Äô√©tat

On d√©finit une table `dp[i]` o√π :

- `i` est une **somme** (allant de `0` √† `S`)
- `dp[i]` est le **nombre de fa√ßons de former la somme `i`**

### Cas de base

- `dp[0] = 1` : il y a **1 seule fa√ßon de rendre 0** (en ne prenant rien)

### Formule de transition

Pour chaque **pi√®ce `c`**, on met √† jour tous les `dp[i]` avec `i ‚â• c` :

```
dp[i] += dp[i - c]
```

‚û° Cela revient √† dire : pour chaque montant `i`, on ajoute toutes les fa√ßons d‚Äôobtenir `i - c`, puis on ajoute une pi√®ce de valeur `c`.

### Impl√©mentation (bottom-up)

```python
def rendu_de_monnaie(pieces, S):
    dp = [0] * (S + 1)
    dp[0] = 1

    for piece in pieces:
        for i in range(piece, S + 1):
            dp[i] += dp[i - piece]

    return dp[S]
```

### Exemple concret

```python
pieces = [1, 2, 5]
S = 5

print(rendu_de_monnaie(pieces, S))  # R√©sultat : 4
```

üü¢ **Explication :**
Les 4 fa√ßons de rendre 5 :

- 1 + 1 + 1 + 1 + 1
- 1 + 1 + 1 + 2
- 1 + 2 + 2
- 5

### Visualisation du tableau `dp`

Pour `S = 5`, les valeurs successives de `dp` sont :

| i (somme) | 0 | 1 | 2 | 3 | 4 | 5 |
||||||||
| dp[i] | 1 | 1 | 2 | 2 | 3 | 4 |

> La valeur finale `dp[5] = 4` correspond au nombre total de combinaisons possibles.

### Complexit√©

| Aspect | Valeur |
||-|
| Temps | `O(n √ó S)` |
| M√©moire | `O(S)` |
| Type | Programmation dynamique 1D |
| Particularit√© | L‚Äôordre des pi√®ces **n‚Äôinfluence pas** le r√©sultat |

### Variante possible

Si on cherche le **nombre minimal de pi√®ces** n√©cessaires pour rendre une somme `S`, on change l‚Äôobjectif du tableau :

- `dp[i] = min(dp[i], dp[i - c] + 1)`
- Initialisation : `dp[0] = 0`, `dp[i>0] = +‚àû`

Ce probl√®me devient alors un **probl√®me d‚Äôoptimisation**, et la logique change l√©g√®rement.

## 7. √âtude de complexit√©

La complexit√© d‚Äôun algorithme en programmation dynamique d√©pend principalement :

- du **nombre total d'√©tats diff√©rents** √† √©valuer (taille de la table `dp`)
- du **temps n√©cessaire pour calculer chaque √©tat**
- de la **structure choisie** (top-down ou bottom-up)

### 7.1. Complexit√© temporelle

| Approche                   | Description                                           | Complexit√©                             |
| -------------------------- | ----------------------------------------------------- | -------------------------------------- |
| **Top-down (m√©mo√Øsation)** | Appels r√©cursifs + cache des r√©sultats interm√©diaires | `O(nombre d‚Äô√©tats r√©ellement visit√©s)` |
| **Bottom-up (tabulaire)**  | Calcul it√©ratif dans un tableau                       | `O(taille du tableau dp)`              |

#### üìå Exemples concrets

| Probl√®me                        | √âtats (dp[i]) ou dp[i][j]        | Complexit√© en temps |
| ------------------------------- | -------------------------------- | ------------------- |
| Fibonacci                       | `dp[i]` avec `i ‚àà [0..n]`        | `O(n)`              |
| Sac √† dos 0/1                   | `dp[i][j]` avec `i ‚â§ n`, `j ‚â§ W` | `O(n √ó W)`          |
| Rendu de monnaie (nb fa√ßons)    | `dp[i]` avec `i ‚àà [0..S]`        | `O(n √ó S)`          |
| Rendu de monnaie (min pi√®ces)   | `dp[i]` avec `i ‚àà [0..S]`        | `O(n √ó S)`          |
| Distance d‚Äô√©dition (alignement) | `dp[i][j]` avec `i ‚â§ n`, `j ‚â§ m` | `O(n √ó m)`          |

### 7.2. Complexit√© spatiale

La m√©moire utilis√©e d√©pend du **nombre d‚Äô√©tats qu‚Äôon conserve**.

| Approche            | Structure typique             | Complexit√© m√©moire                 |
| ------------------- | ----------------------------- | ---------------------------------- |
| Top-down            | Cache (ex : dict)             | `O(n)` ou `O(n √ó k)` selon l‚Äô√©tat  |
| Bottom-up classique | Tableau `dp[i]` ou `dp[i][j]` | `O(n)`, `O(n √ó W)`, etc.           |
| Optimis√© (1D)       | R√©duction ligne/colonne       | `O(W)` ou `O(S)` parfois suffisant |

> üí° De nombreux probl√®mes permettent une **optimisation m√©moire** en n'utilisant **qu‚Äôune ligne ou une colonne √† la fois**, si les d√©pendances le permettent.

### 7.3. Remarques importantes

- Le **top-down** peut √™tre plus simple √† √©crire mais consomme plus de **pile r√©cursive** (‚Üí risque de stack overflow si `n` est grand).
- Le **bottom-up** est souvent plus rapide et mieux ma√Ætris√© en pratique.
- Il est important de bien **d√©finir l‚Äô√©tat** (`dp[...]`) et **d‚Äôanalyser sa dimension** pour estimer la complexit√© globale.
